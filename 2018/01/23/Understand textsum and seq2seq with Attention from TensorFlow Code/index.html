<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Understand textsum and seq2seq with Attention from TensorFlow Code | What&#39;s life without whimsy?</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="This article will peel textsum algorithm and seq2seq with attention mechanism based on the project on TensorFlow/models into very little detail. (tensorflow/models/research/textsum)And here is the cod">
<meta name="keywords" content="Neural Network, TensorFlow">
<meta property="og:type" content="article">
<meta property="og:title" content="Understand textsum and seq2seq with Attention from TensorFlow Code">
<meta property="og:url" content="http://yoursite.com/2018/01/23/Understand textsum and seq2seq with Attention from TensorFlow Code/index.html">
<meta property="og:site_name" content="What&#39;s life without whimsy?">
<meta property="og:description" content="This article will peel textsum algorithm and seq2seq with attention mechanism based on the project on TensorFlow/models into very little detail. (tensorflow/models/research/textsum)And here is the cod">
<meta property="og:updated_time" content="2018-02-05T15:26:11.417Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Understand textsum and seq2seq with Attention from TensorFlow Code">
<meta name="twitter:description" content="This article will peel textsum algorithm and seq2seq with attention mechanism based on the project on TensorFlow/models into very little detail. (tensorflow/models/research/textsum)And here is the cod">
  
    <link rel="alternative" href="/atom.xml" title="What&#39;s life without whimsy?" type="application/atom+xml">
  
  
    <link rel="icon" href="/2017-03-04_14-26-51.jpg">
  
  <link rel="stylesheet" href="/css/style.css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div class="mobile-nav-panel">
	<i class="icon-reorder icon-large"></i>
</div>
<header id="header">
	<h1 class="blog-title">
		<a href="/">What&#39;s life without whimsy?</a>
	</h1>
	<nav class="nav">
		<ul>
			<li><a href="/">Home</a></li><li><a href="/resume">About Me</a></li><li><a href="/archives">Archives</a></li>
			<li><a id="nav-search-btn" class="nav-icon" title="Search"></a></li>
			<li><a href="/atom.xml" id="nav-rss-link" class="nav-icon" title="RSS Feed"></a></li>
		</ul>
	</nav>
	<div id="search-form-wrap">
		<form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
	</div>
</header>
    <div id="main">
      <article id="post-Understand textsum and seq2seq with Attention from TensorFlow Code" class="post">
	<footer class="entry-meta-header">
		<span class="meta-elements date">
			<a href="/2018/01/23/Understand textsum and seq2seq with Attention from TensorFlow Code/" class="article-date">
  <time datetime="2018-01-23T13:40:21.000Z" itemprop="datePublished">2018-01-23</time>
</a>
		</span>
		<span class="meta-elements author">Yuanhanqing Huang</span>
		<div class="commentscount">
			
		</div>
	</footer>
	
	<header class="entry-header">
		
  
    <h1 class="article-title entry-title" itemprop="name">
      Understand textsum and seq2seq with Attention from TensorFlow Code
    </h1>
  

	</header>
	<div class="entry-content">
		
    	<p>This article will peel textsum algorithm and seq2seq with attention mechanism based on the project on TensorFlow/models into very little detail. (<a href="https://github.com/tensorflow/models/tree/master/research/textsum" target="_blank" rel="external">tensorflow/models/research/textsum</a>)And here is the code of my <a href="https://github.com/EstelleHuang666/textsum_tensorflow_usr" target="_blank" rel="external">personal realization</a> of seq2seq with Attention based on the official code, and it turns out to converge quite well.  </p>
<h2 id="Transfer-Text-into-bin-Type"><a href="#Transfer-Text-into-bin-Type" class="headerlink" title="Transfer Text into .bin Type"></a>Transfer Text into .bin Type</h2><p>This part is written in textsum_data_convert.py. The inputs are text file, encoded in utf-8. It has two functions.</p>
<ol>
<li>It outputs .bin file which transform the original unstructured text file into structured .bin file. </li>
<li>It outputs vocab file which counts frequncy of certain word in text files and stores the word as well as its frequency.<br>As for the first function, we have: </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">def _convert_files_to_binary(input_filenames, output_filename):</div><div class="line">  with open(output_filename, &apos;wb&apos;) as writer:</div><div class="line">    for filename in input_filenames:</div><div class="line">      with open(filename, &apos;r&apos;) as f:</div><div class="line">        document = f.read()</div><div class="line">    </div><div class="line">      document_parts = document.split(&apos;\n&apos;, 1)</div><div class="line">      assert len(document_parts) == 2</div><div class="line">    </div><div class="line">      title = &apos;&lt;d&gt;&lt;p&gt;&lt;s&gt;&apos; + document_parts[0] + &apos;&lt;/s&gt;&lt;/p&gt;&lt;/d&gt;&apos;</div><div class="line">      # encode the title into the form of (&apos;UTF-8&apos;), otherwise in tf_example.features.feature[].bytes_list.value.extend()</div><div class="line">      # will report an error.</div><div class="line">      title = title.encode(&apos;utf8&apos;)</div><div class="line">      </div><div class="line">      # body = document_parts[1].decode(&apos;utf8&apos;).replace(&apos;\n&apos;, &apos; &apos;).replace(&apos;\t&apos;, &apos; &apos;)</div><div class="line">      # AttributeError: &apos;str&apos; object has no attribute &apos;decode&apos; -&gt; by Murphy 02.Jan.18</div><div class="line">      try:</div><div class="line">        body = document_parts[1].decode(&apos;utf8&apos;).replace(&apos;\n&apos;, &apos; &apos;).replace(&apos;\t&apos;, &apos; &apos;)</div><div class="line">      except:</div><div class="line">        body = document_parts[1].replace(&apos;\n&apos;, &apos; &apos;).replace(&apos;\t&apos;, &apos; &apos;)</div><div class="line">      sentences = sent_tokenize(body)</div><div class="line">      body = &apos;&lt;d&gt;&lt;p&gt;&apos; + &apos; &apos;.join([&apos;&lt;s&gt;&apos; + sentence + &apos;&lt;/s&gt;&apos; for sentence in sentences]) + &apos;&lt;/p&gt;&lt;/d&gt;&apos;</div><div class="line">      body = body.encode(&apos;utf8&apos;)</div><div class="line">    </div><div class="line">      tf_example = example_pb2.Example()</div><div class="line">      tf_example.features.feature[&apos;article&apos;].bytes_list.value.extend([body])</div><div class="line">      tf_example.features.feature[&apos;abstract&apos;].bytes_list.value.extend([title])</div><div class="line">      tf_example_str = tf_example.SerializeToString()</div><div class="line">      str_len = len(tf_example_str)</div><div class="line">      writer.write(struct.pack(&apos;q&apos;, str_len))</div><div class="line">      writer.write(struct.pack(&apos;%ds&apos; % str_len, tf_example_str))</div></pre></td></tr></table></figure>
<p>It processes all the text files under the path “./data/cnn/stories”. These text files are consist of two parts, “title” and “body” (also known as “abstract” and “article”), which are seperated by the first “\n”. For title and body, both of them are decorated with prefix <d><p><s> and postfix </s></p></d>. Moreover, body part is seperated into sentences by the fucntion sent_tokenize from nltk package. Every sentence in body is decorated with prefix <s> and postfix </s>. The “/n”s in body are replaced by “/t”s. Then, the processed “title” and “body” are writen into text file through the format of example_pb2.Example(), which is a format imported from tensorflow.core.example. It can simply be preceived as a storing format without special meaning. If you like, you could write your own format to replace example_pb2.Example().</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">def _text_to_vocabulary(input_directories, vocabulary_filename, max_words=200000):</div><div class="line">  filenames = _get_filenames(input_directories)</div><div class="line">    </div><div class="line">  counter = collections.Counter()</div><div class="line">    </div><div class="line">  for filename in filenames:</div><div class="line">    with open(filename, &apos;r&apos;) as f:</div><div class="line">      document = f.read()</div><div class="line">    </div><div class="line">    words = document.split()</div><div class="line">    counter.update(words)</div><div class="line"></div><div class="line">  with open(vocabulary_filename, &apos;w&apos;) as writer:</div><div class="line">    for word, count in counter.most_common(max_words - 2):</div><div class="line">      writer.write(word + &apos; &apos; + str(count) + &apos;\n&apos;)</div><div class="line">    writer.write(&apos;&lt;s&gt; 0\n&apos;)</div><div class="line">    writer.write(&apos;&lt;/s&gt; 0\n&apos;)</div><div class="line">    writer.write(&apos;&lt;UNK&gt; 0\n&apos;)</div><div class="line">    writer.write(&apos;&lt;PAD&gt; 0\n&apos;)</div></pre></td></tr></table></figure>
<p>collections.Counter() works as a collector (or you can see as a dictionary) in the format of {“word1”: frequency1 , “word2”: frequency2 ,…}. By executing “.update” operation and pass a list of words into the Counter, you could let this class automatically update the word-frequency dictionary. At the end, it adds some symbols with special meaning to the Counter. (<s> </s> denote the start and the end of a sentence; <pad> is used to padding the blank to make all of the sentences have same length) </pad></p>
<h2 id="batcher-reader-management-of-input-data"><a href="#batcher-reader-management-of-input-data" class="headerlink" title="batcher_reader: management of input data"></a>batcher_reader: management of input data</h2><p>First, let’s have a glance of the major part of function main:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">vocab = data.Vocab(FLAGS.vocab_path, 1000000)</div><div class="line">  # Check for presence of required special tokens.</div><div class="line">  assert vocab.CheckVocab(data.PAD_TOKEN) &gt; 0</div><div class="line">  assert vocab.CheckVocab(data.UNKNOWN_TOKEN) &gt;= 0</div><div class="line">  assert vocab.CheckVocab(data.SENTENCE_START) &gt; 0</div><div class="line">  assert vocab.CheckVocab(data.SENTENCE_END) &gt; 0</div><div class="line"></div><div class="line">  batch_size = 4</div><div class="line">  if FLAGS.mode == &apos;decode&apos;:</div><div class="line">    batch_size = FLAGS.beam_size</div><div class="line"></div><div class="line">  hps = seq2seq_attention_model.HParams(</div><div class="line">      mode=FLAGS.mode,  # train, eval, decode</div><div class="line">      min_lr=0.001,  # min learning rate.</div><div class="line">      lr=0.015,  # learning rate</div><div class="line">      batch_size=batch_size,</div><div class="line">      enc_layers=1,</div><div class="line">      enc_timesteps=800,</div><div class="line">      dec_timesteps=50,</div><div class="line">      min_input_len=2,  # discard articles/summaries &lt; than this</div><div class="line">      num_hidden=256,  # for rnn cell</div><div class="line">      emb_dim=128,  # If 0, don&apos;t use embedding</div><div class="line">      max_grad_norm=2,</div><div class="line">      num_softmax_samples=4096)  # If 0, no sampled softmax.</div><div class="line"></div><div class="line">  batcher = batch_reader.Batcher(</div><div class="line">      FLAGS.data_path, vocab, hps, FLAGS.article_key,</div><div class="line">      FLAGS.abstract_key, FLAGS.max_article_sentences,</div><div class="line">      FLAGS.max_abstract_sentences, bucketing=FLAGS.use_bucketing,</div><div class="line">      truncate_input=FLAGS.truncate_input)</div><div class="line">  tf.set_random_seed(FLAGS.random_seed)</div><div class="line"></div><div class="line">  if hps.mode == &apos;train&apos;:</div><div class="line">    model = seq2seq_attention_model.Seq2SeqAttentionModel(</div><div class="line">        hps, vocab, num_gpus=FLAGS.num_gpus)</div><div class="line">    _Train(model, batcher)</div><div class="line">  elif hps.mode == &apos;eval&apos;:</div><div class="line">    model = seq2seq_attention_model.Seq2SeqAttentionModel(</div><div class="line">        hps, vocab, num_gpus=FLAGS.num_gpus)</div><div class="line">    _Eval(model, batcher, vocab=vocab)</div><div class="line">  elif hps.mode == &apos;decode&apos;:</div><div class="line">    decode_mdl_hps = hps</div><div class="line">    # Only need to restore the 1st step and reuse it since</div><div class="line">    # we keep and feed in state for each step&apos;s output.</div><div class="line">    decode_mdl_hps = hps._replace(dec_timesteps=1)</div><div class="line">    model = seq2seq_attention_model.Seq2SeqAttentionModel(</div><div class="line">        decode_mdl_hps, vocab, num_gpus=FLAGS.num_gpus)</div><div class="line">    decoder = seq2seq_attention_decode.BSDecoder(model, batcher, hps, vocab)</div><div class="line">    decoder.DecodeLoop()</div></pre></td></tr></table></figure></p>
<p>We can see that main function do the following steps:</p>
<ol>
<li>read in hyperparameters;</li>
<li>use batch_reader to manage the data;</li>
<li>execute training/evaluating/decoding process.</li>
</ol>
<p>For batch_reader, it only contains one class: Batcher. It has following methods:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">class Batcher(object):</div><div class="line">	def NextBatch(self):</div><div class="line"></div><div class="line">	def _FillInputQueue(self):</div><div class="line"></div><div class="line">	def _FillBucketInputQueue(self):</div><div class="line"></div><div class="line">	def _WatchThreads(self):</div><div class="line"></div><div class="line">	def _TextGenerator(self, example_gen):</div><div class="line"></div><div class="line">	def _GetExFeatureText(self, ex, key):</div><div class="line"></div></pre></td></tr></table></figure></p>

    
	</div>
	<footer class="entry-footer">
		<div class="entry-meta-footer">
			<span class="category">
				
			</span>
			<span class="tags">
				
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Neural-Network-TensorFlow/">Neural Network, TensorFlow</a></li></ul>

			</span>
		</div>
	</footer>
	
    
<nav id="article-nav">
  
  
    <a href="/2018/01/04/Variational Inference/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          Variational Inference
        
      </div>
    </a>
  
</nav>

  
</article>




    </div>
    <div class="mb-search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:yoursite.com">
  </form>
</div>
<footer id="footer">
	<h1 class="footer-blog-title">
		<a href="/">What&#39;s life without whimsy?</a>
	</h1>
	<span class="copyright">
		&copy; 2018 Yuanhanqing Huang<br>
		Modify from <a href="http://sanographix.github.io/tumblr/apollo/" target="_blank">Apollo</a> theme, designed by <a href="http://www.sanographix.net/" target="_blank">SANOGRAPHIX.NET</a><br>
		Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
	</span>
</footer>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>