<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Neural Network, TensorFlow," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="This article will peel textsum algorithm and seq2seq with attention mechanism based on the project on TensorFlow/models into very little detail. (tensorflow/models/research/textsum)And here is the cod">
<meta name="keywords" content="Neural Network, TensorFlow">
<meta property="og:type" content="article">
<meta property="og:title" content="Understand textsum and seq2seq with Attention from TensorFlow Code">
<meta property="og:url" content="http://yoursite.com/2018/01/23/Understand textsum and seq2seq with Attention from TensorFlow Code/index.html">
<meta property="og:site_name" content="What&#39;s life without whimsy?">
<meta property="og:description" content="This article will peel textsum algorithm and seq2seq with attention mechanism based on the project on TensorFlow/models into very little detail. (tensorflow/models/research/textsum)And here is the cod">
<meta property="og:updated_time" content="2018-02-06T08:23:30.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Understand textsum and seq2seq with Attention from TensorFlow Code">
<meta name="twitter:description" content="This article will peel textsum algorithm and seq2seq with attention mechanism based on the project on TensorFlow/models into very little detail. (tensorflow/models/research/textsum)And here is the cod">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/01/23/Understand textsum and seq2seq with Attention from TensorFlow Code/"/>





  <title>Understand textsum and seq2seq with Attention from TensorFlow Code | What's life without whimsy?</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">What's life without whimsy?</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Homepage of Yuanhanqing Huang</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/23/Understand textsum and seq2seq with Attention from TensorFlow Code/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yuanhanqing Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="What's life without whimsy?">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Understand textsum and seq2seq with Attention from TensorFlow Code</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Ver√∂ffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-23T21:40:21-05:00">
                2018-01-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>This article will peel textsum algorithm and seq2seq with attention mechanism based on the project on TensorFlow/models into very little detail. (<a href="https://github.com/tensorflow/models/tree/master/research/textsum" target="_blank" rel="external">tensorflow/models/research/textsum</a>)And here is the code of my <a href="https://github.com/EstelleHuang666/textsum_tensorflow_usr" target="_blank" rel="external">personal realization</a> of seq2seq with Attention based on the official code, and it turns out to converge quite well.  </p>
<h2 id="Transfer-Text-into-bin-Type"><a href="#Transfer-Text-into-bin-Type" class="headerlink" title="Transfer Text into .bin Type"></a>Transfer Text into .bin Type</h2><p>This part is written in textsum_data_convert.py. The inputs are text file, encoded in utf-8. It has two functions.</p>
<ol>
<li>It outputs .bin file which transform the original unstructured text file into structured .bin file. </li>
<li>It outputs vocab file which counts frequncy of certain word in text files and stores the word as well as its frequency.<br>As for the first function, we have: </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">def _convert_files_to_binary(input_filenames, output_filename):</div><div class="line">  with open(output_filename, &apos;wb&apos;) as writer:</div><div class="line">    for filename in input_filenames:</div><div class="line">      with open(filename, &apos;r&apos;) as f:</div><div class="line">        document = f.read()</div><div class="line">    </div><div class="line">      document_parts = document.split(&apos;\n&apos;, 1)</div><div class="line">      assert len(document_parts) == 2</div><div class="line">    </div><div class="line">      title = &apos;&lt;d&gt;&lt;p&gt;&lt;s&gt;&apos; + document_parts[0] + &apos;&lt;/s&gt;&lt;/p&gt;&lt;/d&gt;&apos;</div><div class="line">      # encode the title into the form of (&apos;UTF-8&apos;), otherwise in tf_example.features.feature[].bytes_list.value.extend()</div><div class="line">      # will report an error.</div><div class="line">      title = title.encode(&apos;utf8&apos;)</div><div class="line">      </div><div class="line">      # body = document_parts[1].decode(&apos;utf8&apos;).replace(&apos;\n&apos;, &apos; &apos;).replace(&apos;\t&apos;, &apos; &apos;)</div><div class="line">      # AttributeError: &apos;str&apos; object has no attribute &apos;decode&apos; -&gt; by Murphy 02.Jan.18</div><div class="line">      try:</div><div class="line">        body = document_parts[1].decode(&apos;utf8&apos;).replace(&apos;\n&apos;, &apos; &apos;).replace(&apos;\t&apos;, &apos; &apos;)</div><div class="line">      except:</div><div class="line">        body = document_parts[1].replace(&apos;\n&apos;, &apos; &apos;).replace(&apos;\t&apos;, &apos; &apos;)</div><div class="line">      sentences = sent_tokenize(body)</div><div class="line">      body = &apos;&lt;d&gt;&lt;p&gt;&apos; + &apos; &apos;.join([&apos;&lt;s&gt;&apos; + sentence + &apos;&lt;/s&gt;&apos; for sentence in sentences]) + &apos;&lt;/p&gt;&lt;/d&gt;&apos;</div><div class="line">      body = body.encode(&apos;utf8&apos;)</div><div class="line">    </div><div class="line">      tf_example = example_pb2.Example()</div><div class="line">      tf_example.features.feature[&apos;article&apos;].bytes_list.value.extend([body])</div><div class="line">      tf_example.features.feature[&apos;abstract&apos;].bytes_list.value.extend([title])</div><div class="line">      tf_example_str = tf_example.SerializeToString()</div><div class="line">      str_len = len(tf_example_str)</div><div class="line">      writer.write(struct.pack(&apos;q&apos;, str_len))</div><div class="line">      writer.write(struct.pack(&apos;%ds&apos; % str_len, tf_example_str))</div></pre></td></tr></table></figure>
<p>It processes all the text files under the path ‚Äú./data/cnn/stories‚Äù. These text files are consist of two parts, ‚Äútitle‚Äù and ‚Äúbody‚Äù (also known as ‚Äúabstract‚Äù and ‚Äúarticle‚Äù), which are seperated by the first ‚Äú\n‚Äù. For title and body, both of them are decorated with prefix &lt;\d&gt;&lt;\p&gt;&lt;\s&gt; and postfix &lt;/\s&gt;&lt;/\p&gt;&lt;/\d&gt;. Moreover, body part is seperated into sentences by the fucntion sent_tokenize from nltk package. Every sentence in body is decorated with prefix &lt;\s&gt; and postfix &lt;/\s&gt;. The ‚Äú/n‚Äùs in body are replaced by ‚Äú/t‚Äùs. Then, the processed ‚Äútitle‚Äù and ‚Äúbody‚Äù are writen into text file through the format of example_pb2.Example(), which is a format imported from tensorflow.core.example. It can simply be preceived as a storing format without special meaning. If you like, you could write your own format to replace example_pb2.Example().</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">def _text_to_vocabulary(input_directories, vocabulary_filename, max_words=200000):</div><div class="line">  filenames = _get_filenames(input_directories)</div><div class="line">    </div><div class="line">  counter = collections.Counter()</div><div class="line">    </div><div class="line">  for filename in filenames:</div><div class="line">    with open(filename, &apos;r&apos;) as f:</div><div class="line">      document = f.read()</div><div class="line">    </div><div class="line">    words = document.split()</div><div class="line">    counter.update(words)</div><div class="line"></div><div class="line">  with open(vocabulary_filename, &apos;w&apos;) as writer:</div><div class="line">    for word, count in counter.most_common(max_words - 2):</div><div class="line">      writer.write(word + &apos; &apos; + str(count) + &apos;\n&apos;)</div><div class="line">    writer.write(&apos;&lt;s&gt; 0\n&apos;)</div><div class="line">    writer.write(&apos;&lt;/s&gt; 0\n&apos;)</div><div class="line">    writer.write(&apos;&lt;UNK&gt; 0\n&apos;)</div><div class="line">    writer.write(&apos;&lt;PAD&gt; 0\n&apos;)</div></pre></td></tr></table></figure>
<p>collections.Counter() works as a collector (or you can see as a dictionary) in the format of {‚Äúword1‚Äù: frequency1 , ‚Äúword2‚Äù: frequency2 ,‚Ä¶}. By executing ‚Äú.update‚Äù operation and pass a list of words into the Counter, you could let this class automatically update the word-frequency dictionary. At the end, it adds some symbols with special meaning to the Counter. (&lt;\s&gt; &lt;/\s&gt; denote the start and the end of a sentence; <pad> is used to padding the blank to make all of the sentences have same length) </pad></p>
<h2 id="batcher-reader-management-of-input-data"><a href="#batcher-reader-management-of-input-data" class="headerlink" title="batcher_reader: management of input data"></a>batcher_reader: management of input data</h2><p>First, let‚Äôs have a glance of the major part of function main:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">vocab = data.Vocab(FLAGS.vocab_path, 1000000)</div><div class="line">  # Check for presence of required special tokens.</div><div class="line">  assert vocab.CheckVocab(data.PAD_TOKEN) &gt; 0</div><div class="line">  assert vocab.CheckVocab(data.UNKNOWN_TOKEN) &gt;= 0</div><div class="line">  assert vocab.CheckVocab(data.SENTENCE_START) &gt; 0</div><div class="line">  assert vocab.CheckVocab(data.SENTENCE_END) &gt; 0</div><div class="line"></div><div class="line">  batch_size = 4</div><div class="line">  if FLAGS.mode == &apos;decode&apos;:</div><div class="line">    batch_size = FLAGS.beam_size</div><div class="line"></div><div class="line">  hps = seq2seq_attention_model.HParams(</div><div class="line">      mode=FLAGS.mode,  # train, eval, decode</div><div class="line">      min_lr=0.001,  # min learning rate.</div><div class="line">      lr=0.015,  # learning rate</div><div class="line">      batch_size=batch_size,</div><div class="line">      enc_layers=1,</div><div class="line">      enc_timesteps=800,</div><div class="line">      dec_timesteps=50,</div><div class="line">      min_input_len=2,  # discard articles/summaries &lt; than this</div><div class="line">      num_hidden=256,  # for rnn cell</div><div class="line">      emb_dim=128,  # If 0, don&apos;t use embedding</div><div class="line">      max_grad_norm=2,</div><div class="line">      num_softmax_samples=4096)  # If 0, no sampled softmax.</div><div class="line"></div><div class="line">  batcher = batch_reader.Batcher(</div><div class="line">      FLAGS.data_path, vocab, hps, FLAGS.article_key,</div><div class="line">      FLAGS.abstract_key, FLAGS.max_article_sentences,</div><div class="line">      FLAGS.max_abstract_sentences, bucketing=FLAGS.use_bucketing,</div><div class="line">      truncate_input=FLAGS.truncate_input)</div><div class="line">  tf.set_random_seed(FLAGS.random_seed)</div><div class="line"></div><div class="line">  if hps.mode == &apos;train&apos;:</div><div class="line">    model = seq2seq_attention_model.Seq2SeqAttentionModel(</div><div class="line">        hps, vocab, num_gpus=FLAGS.num_gpus)</div><div class="line">    _Train(model, batcher)</div><div class="line">  elif hps.mode == &apos;eval&apos;:</div><div class="line">    model = seq2seq_attention_model.Seq2SeqAttentionModel(</div><div class="line">        hps, vocab, num_gpus=FLAGS.num_gpus)</div><div class="line">    _Eval(model, batcher, vocab=vocab)</div><div class="line">  elif hps.mode == &apos;decode&apos;:</div><div class="line">    decode_mdl_hps = hps</div><div class="line">    # Only need to restore the 1st step and reuse it since</div><div class="line">    # we keep and feed in state for each step&apos;s output.</div><div class="line">    decode_mdl_hps = hps._replace(dec_timesteps=1)</div><div class="line">    model = seq2seq_attention_model.Seq2SeqAttentionModel(</div><div class="line">        decode_mdl_hps, vocab, num_gpus=FLAGS.num_gpus)</div><div class="line">    decoder = seq2seq_attention_decode.BSDecoder(model, batcher, hps, vocab)</div><div class="line">    decoder.DecodeLoop()</div></pre></td></tr></table></figure></p>
<p>We can see that main function do the following steps:</p>
<ol>
<li>read in hyperparameters;</li>
<li>use batch_reader to manage the data;</li>
<li>execute training/evaluating/decoding process.</li>
</ol>
<p>For batch_reader, it only contains one class: Batcher. It has following methods:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">class Batcher(object):</div><div class="line">	def NextBatch(self):</div><div class="line"></div><div class="line">	def _FillInputQueue(self):</div><div class="line"></div><div class="line">	def _FillBucketInputQueue(self):</div><div class="line"></div><div class="line">	def _WatchThreads(self):</div><div class="line"></div><div class="line">	def _TextGenerator(self, example_gen):</div><div class="line"></div><div class="line">	def _GetExFeatureText(self, ex, key):</div><div class="line"></div></pre></td></tr></table></figure></p>
<p>We are gonna to dissect these code one by one:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line">def _FillInputQueue(self):</div><div class="line">	&quot;&quot;&quot;Fill input queue with ModelInput.&quot;&quot;&quot;</div><div class="line">	start_id = self._vocab.WordToId(data.SENTENCE_START)</div><div class="line">	end_id = self._vocab.WordToId(data.SENTENCE_END)</div><div class="line">	pad_id = self._vocab.WordToId(data.PAD_TOKEN)</div><div class="line">	input_gen = self._TextGenerator(data.ExampleGen(self._data_path))</div><div class="line">	while True:</div><div class="line">	  (article, abstract) = six.next(input_gen)</div><div class="line">	  article_sentences = [sent.strip() for sent in</div><div class="line">	                       data.ToSentences(article.decode(&apos;utf-8&apos;), include_token=False)]</div><div class="line">	  abstract_sentences = [sent.strip() for sent in</div><div class="line">	                        data.ToSentences(abstract.decode(&apos;utf-8&apos;), include_token=False)]</div><div class="line"></div><div class="line">	  enc_inputs = []</div><div class="line">	  # Use the &lt;s&gt; as the &lt;GO&gt; symbol for decoder inputs.</div><div class="line">	  dec_inputs = [start_id]</div><div class="line"></div><div class="line">	  # Convert first N sentences to word IDs, stripping existing &lt;s&gt; and &lt;/s&gt;.</div><div class="line">	  for i in xrange(min(self._max_article_sentences,</div><div class="line">	                      len(article_sentences))):</div><div class="line">	    enc_inputs += data.GetWordIds(article_sentences[i], self._vocab)</div><div class="line">	  for i in xrange(min(self._max_abstract_sentences,</div><div class="line">	                      len(abstract_sentences))):</div><div class="line">	    dec_inputs += data.GetWordIds(abstract_sentences[i], self._vocab)</div><div class="line"></div><div class="line">	  # Filter out too-short input</div><div class="line">	  if (len(enc_inputs) &lt; self._hps.min_input_len or</div><div class="line">	      len(dec_inputs) &lt; self._hps.min_input_len):</div><div class="line">	    tf.logging.warning(&apos;Drop an example - too short.\nenc:%d\ndec:%d&apos;,</div><div class="line">	                       len(enc_inputs), len(dec_inputs))</div><div class="line">	    continue</div><div class="line"></div><div class="line">	  # If we&apos;re not truncating input, throw out too-long input</div><div class="line">	  if not self._truncate_input:</div><div class="line">	    if (len(enc_inputs) &gt; self._hps.enc_timesteps or</div><div class="line">	        len(dec_inputs) &gt; self._hps.dec_timesteps):</div><div class="line">	      tf.logging.warning(&apos;Drop an example - too long.\nenc:%d\ndec:%d&apos;,</div><div class="line">	                         len(enc_inputs), len(dec_inputs))</div><div class="line">	      continue</div><div class="line">	  # If we are truncating input, do so if necessary</div><div class="line">	  else:</div><div class="line">	    if len(enc_inputs) &gt; self._hps.enc_timesteps:</div><div class="line">	      enc_inputs = enc_inputs[:self._hps.enc_timesteps]</div><div class="line">	    if len(dec_inputs) &gt; self._hps.dec_timesteps:</div><div class="line">	      dec_inputs = dec_inputs[:self._hps.dec_timesteps]</div><div class="line"></div><div class="line">	  # targets is dec_inputs without &lt;s&gt; at beginning, plus &lt;/s&gt; at end</div><div class="line">	  targets = dec_inputs[1:]</div><div class="line">	  targets.append(end_id)</div><div class="line"></div><div class="line">	  # Now len(enc_inputs) should be &lt;= enc_timesteps, and</div><div class="line">	  # len(targets) = len(dec_inputs) should be &lt;= dec_timesteps</div><div class="line"></div><div class="line">	  enc_input_len = len(enc_inputs)</div><div class="line">	  dec_output_len = len(targets)</div><div class="line"></div><div class="line">	  # Pad if necessary</div><div class="line">	  while len(enc_inputs) &lt; self._hps.enc_timesteps:</div><div class="line">	    enc_inputs.append(pad_id)</div><div class="line">	  while len(dec_inputs) &lt; self._hps.dec_timesteps:</div><div class="line">	    dec_inputs.append(end_id)</div><div class="line">	  while len(targets) &lt; self._hps.dec_timesteps:</div><div class="line">	    targets.append(end_id)</div><div class="line"></div><div class="line">	  element = ModelInput(enc_inputs, dec_inputs, targets, enc_input_len,</div><div class="line">	                       dec_output_len, &apos; &apos;.join(article_sentences),</div><div class="line">	                       &apos; &apos;.join(abstract_sentences))</div><div class="line">	  self._input_queue.put(element)</div></pre></td></tr></table></figure></p>
<p>self._vocab is a class defined in data.py. To understand it briefly, you can view it as composed of two dictionary: _word_to_id and _id_to_word. In addition, this class provides methods related to ‚Äúword to id‚Äù and ‚Äúid to word‚Äù processes. As for self._TextGenerator method, it uses yield instead of return to deal with the huge memory usage. To master yield, you must understand that when you call the function, the code you have written in the function body does not run. The function only returns the generator object. To understand better, please refer to <a href="https://pythontips.com/2013/09/29/the-python-yield-keyword-explained/" target="_blank" rel="external">this article</a>. </p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">def _TextGenerator(self, example_gen):</div><div class="line">	&quot;&quot;&quot;Generates article and abstract text from tf.Example.&quot;&quot;&quot;</div><div class="line">	while True:</div><div class="line">	  e = six.next(example_gen)</div><div class="line">	  try:</div><div class="line">	    article_text = self._GetExFeatureText(e, self._article_key) //return ex.features.feature[key].bytes_list.value[0]</div><div class="line">	    abstract_text = self._GetExFeatureText(e, self._abstract_key)</div><div class="line">	  except ValueError:</div><div class="line">	    tf.logging.error(&apos;Failed to get article or abstract from example&apos;)</div><div class="line">	    continue</div><div class="line"></div><div class="line">	  yield (article_text, abstract_text)</div></pre></td></tr></table></figure>
<p>six.next(example_gen) generates a tf.Example type if data of the following sample format. It‚Äôs long, but you only need to have a glanpse of its basic format.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">features &#123;</div><div class="line">  feature &#123;</div><div class="line">    key: &quot;abstract&quot;</div><div class="line">    value &#123;</div><div class="line">      bytes_list &#123;</div><div class="line">        value: &quot;&lt;d&gt;&lt;p&gt;&lt;s&gt;(CNN) -- Republican presidential contender Michele Bachmann defended her position on gay rights, the HPV vaccine and the debt ceiling as she made her debut on \&quot;The Tonight Show with Jay Leno.\&quot;&lt;/s&gt;&lt;/p&gt;&lt;/d&gt;&quot;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  feature &#123;</div><div class="line">    key: &quot;article&quot;</div><div class="line">    value &#123;</div><div class="line">      bytes_list &#123;</div><div class="line">        value: &quot;&lt;d&gt;&lt;p&gt;&lt;s&gt; Bachmann has more usually been the butt of Leno\&apos;s jokes -- a point he made as he thanked her for being a good sport.&lt;/s&gt; &lt;s&gt;\&quot;We\&apos;ve done a million jokes.&lt;/s&gt; &lt;s&gt;Hopefully, you haven\&apos;t been ... watching any of them,\&quot; he said, as she joined him on set.&lt;/s&gt; &lt;s&gt;But Leno largely skipped the jokes Friday as he quizzed the Minnesota congresswoman on her political positions.&lt;/s&gt; &lt;s&gt;First up was the issue of the HPV vaccine, a subject on which Bachmann hit fellow Republican contender and Texas Gov.&lt;/s&gt; &lt;s&gt;Rick Perry hard in this week\&apos;s CNN/Tea Party Republican Debate.&lt;/s&gt; &lt;s&gt;Perry signed an executive order in 2007 that required Texas schoolgirls to receive vaccinations against the sexually transmitted HPV, although it wasn\&apos;t implemented.&lt;/s&gt; &lt;s&gt;Bachmann told Leno that Perry\&apos;s action had been \&quot;an abuse of executive power\&quot; and had sparked concern over \&quot;crony capitalism,\&quot; an apparent reference to the fact that a former Perry aide was a top lobbyist for Merck, the manufacturer for the HPV vaccine.&lt;/s&gt; &lt;s&gt;Merck donated to Perry\&apos;s campaign fund.&lt;/s&gt; &lt;s&gt;She added: \&quot;The concern is that there\&apos;s, you know, potentially side effects that can come with something like that.&lt;/s&gt; &lt;s&gt;But it gives a false sense of assurance to a young woman when she has that that if she\&apos;s sexually active that she doesn\&apos;t have to worry about sexually transmitted diseases.\&quot;&lt;/s&gt; &lt;s&gt;Leno responded: \&quot;Well, I don\&apos;t know if it gives assurance.&lt;/s&gt; &lt;s&gt;It can prevent cervical cancer; correct?\&quot;&lt;/s&gt; &lt;s&gt;He then pressed Bachmann over comments she made earlier this week in which she said a woman had approached the congresswoman to say her daughter had suffered \&quot;mental retardation\&quot; as a result of receiving the vaccination.&lt;/s&gt; &lt;s&gt;There had been no recorded cases of such side effects despite 30 million people receiving the jab, Leno pointed out.&lt;/s&gt; &lt;s&gt;\&quot;I wasn\&apos;t speaking as a doctor.&lt;/s&gt; &lt;s&gt;I wasn\&apos;t speaking as a scientist.&lt;/s&gt; &lt;s&gt;I was just relating what this woman said,\&quot; Bachmann replied.&lt;/s&gt; &lt;s&gt;The former tax attorney and mother of five, who won the Iowa straw poll last month but has seen her poll ratings slide since Perry entered the race, also defended two clinics she runs with her husband, offering what she said was a Christian counseling service.&lt;/s&gt; &lt;s&gt;The clinics have come under fire over claims they use a controversial therapy that encourages gay and lesbian patients to change their sexual orientation.&lt;/s&gt; &lt;s&gt;Asking Bachmann why gay people shouldn\&apos;t have the right to be happily married, Leno said: \&quot;That whole \&apos;pray the gay away\&apos; thing, What?&lt;/s&gt; &lt;s&gt;I don\&apos;t get that.\&quot;&lt;/s&gt; &lt;s&gt;Bachmann said the clinics did not discriminate, but repeated her position that marriage should be between a man and a woman.&lt;/s&gt; &lt;s&gt;Quizzed on her opposition to raising the debt ceiling, Bachmann said she would have taken the same position whether it had been President Barack Obama or George W. Bush in power.&lt;/s&gt; &lt;s&gt;On Afghanistan, Bachmann failed to answer whether she thought American forces should withdraw, but paid tribute to the \&quot;unbelievable job\&quot; done by U.S. service men and women there.&lt;/s&gt; &lt;s&gt;Leno\&apos;s final question was whom Bachmann would pick as a running mate if she wins the Republican nomination, suggesting she might want someone more moderate to balance her views.&lt;/s&gt; &lt;s&gt;She joked: \&quot;Well, you\&apos;re taken.&lt;/s&gt; &lt;s&gt;You don\&apos;t want a cut in pay, so what can I say?\&quot;&lt;/s&gt; &lt;s&gt;Leno replied: \&quot;Well, we\&apos;d probably have an argument over that gay thing.\&quot;&lt;/s&gt; &lt;s&gt;@highlight  Bachmann continues her criticism of Rick Perry over the HPV vaccine  @highlight  Leno presses Bachmann on gay marriage and the counseling clinics she runs  @highlight  The presidential hopeful says her opposition to raising the debt ceiling was not political  @highlight  Bachmann declines to say who her running mate would be&lt;/s&gt;&lt;/p&gt;&lt;/d&gt;&quot;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Neural-Network-TensorFlow/" rel="tag"># Neural Network, TensorFlow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/01/04/Variational Inference/" rel="next" title="Variational Inference">
                <i class="fa fa-chevron-left"></i> Variational Inference
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Inhaltsverzeichnis
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            √úbersicht
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <p class="site-author-name" itemprop="name">Yuanhanqing Huang</p>
            <p class="site-description motion-element" itemprop="description"></p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">Artikel</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">Tags</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Transfer-Text-into-bin-Type"><span class="nav-number">1.</span> <span class="nav-text">Transfer Text into .bin Type</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#batcher-reader-management-of-input-data"><span class="nav-number">2.</span> <span class="nav-text">batcher_reader: management of input data</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yuanhanqing Huang</span>

  
</div>


  <div class="powered-by">Erstellt mit  <a class="theme-link" href="https://hexo.io">Hexo</a></div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">Theme &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]]}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  








  





  

  

  

  

  

  

</body>
</html>
