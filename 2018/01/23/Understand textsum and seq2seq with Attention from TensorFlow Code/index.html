<!DOCTYPE html>
<html class="full-height">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <link rel="stylesheet" href="//cdn.bootcss.com/bulma/0.4.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
  
  <title>Understand textsum and seq2seq with Attention from TensorFlow Code | What&#39;s life without whimsy?</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="This article will peel textsum algorithm and seq2seq with attention mechanism based on the project on TensorFlow/models into very little detail. (tensorflow/models/research/textsum)And here is the cod">
<meta name="keywords" content="Neural Network, TensorFlow">
<meta property="og:type" content="article">
<meta property="og:title" content="Understand textsum and seq2seq with Attention from TensorFlow Code">
<meta property="og:url" content="http://yoursite.com/2018/01/23/Understand textsum and seq2seq with Attention from TensorFlow Code/index.html">
<meta property="og:site_name" content="What&#39;s life without whimsy?">
<meta property="og:description" content="This article will peel textsum algorithm and seq2seq with attention mechanism based on the project on TensorFlow/models into very little detail. (tensorflow/models/research/textsum)And here is the cod">
<meta property="og:updated_time" content="2018-02-06T08:23:30.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Understand textsum and seq2seq with Attention from TensorFlow Code">
<meta name="twitter:description" content="This article will peel textsum algorithm and seq2seq with attention mechanism based on the project on TensorFlow/models into very little detail. (tensorflow/models/research/textsum)And here is the cod">
  
    <link rel="alternate" href="/atom.xml" title="What&#39;s life without whimsy?" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/common.css">
<link rel="stylesheet" href="/css/nav.css">
<link rel="stylesheet" href="/css/layout.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="navbar" class="overflow-hidden">
  <div class="container">
    <nav class="nav">
         <div class="nav-left">
            <a href="/" class="nav-item" style="font-size: 20px;">
              <span class="logo">Yuanhanqing Huang</span>'s Blog
            </a>
         </div>
        <div class="nav-center is-hidden position-relative" id="search_container">
            <div class="nav-item full-width full-height">
                <i class="fa fa-search has-padding" aria-hidden="true"></i>
                <input type="text" id="search_input" class="search-input full-height full-width" placeholder="Search post" autofocus>
                <i id="close_search" class="fa fa-times" aria-hidden="true"></i>
            </div>
            <div id="search_result"></div>
        </div>
        <div class="nav-right nav-menu">
            <a class="nav-item" id="search">
                <i class="fa fa-search" aria-hidden="true"></i>
            </a>
            
            <a class="nav-item" href="/">
                Home
            </a>
            
            <a class="nav-item" href="/Resume">
                Resume
            </a>
            
        </div>
        <span class="nav-toggle" id="navMenuDropdown">
            <span></span>
            <span></span>
            <span></span>
        </span>
        <div class="navbar-menu position-absolute full-width content-box is-hidden-desktop is-flex flex-column center" style="top: 100%;">
            
            <a class="nav-item flex-1" href="/">
                Home
            </a>
            
            <a class="nav-item flex-1" href="/Resume">
                Resume
            </a>
            
        </div>
    </nav>
  </div>
</header>

  <div id="main-wrap" class="position-relative" style="margin-top: 55px;">
      <div class="main-inner-content">
          <!--博文页面-->

<style>
    .header-box {
        height: 370px;
        filter: blur(10px);
        background-size: cover;
        background-color: lightsteelblue;
    }

    .post-box {
        padding: 15px;
        padding-top: 60px;
        min-height: 80vh;
        margin-top: -200px;
        border-radius: 4px;
        background-color: rgba(255,255,255,.8);
    }

    .post-avatar {
        height: 30px;
        width: 30px;
        border-radius: 50%;
    }

    .flow-chart {
        text-align: center;
    }

    img[alt="post-cover"] {
        display: none;
    }
</style>
<header>
    <div id="header_box" class="header-box"></div>
</header>
<section>
    <div class="container post-box">
        <div class="content post-title is-flex center flex-column" style="margin-bottom: 70px; overflow: auto;">
            <h1 class="has-text-centered" style="padding-bottom: 10px; border-bottom: 3px solid #fff">
                <strong>Understand textsum and seq2seq with Attention from TensorFlow Code</strong>
            </h1>
            
            <div class="is-flex align-center">
                <img class="post-avatar" src="/img/personal_pic.jpg">
                <span style="padding:0 10px;"> <span class="sub-title">By</span> Yuanhanqing Huang</span>
                <span class="post-date sub-title">at: 2018-01-23</span>
            </div>
            
                <div>
                    
                         <a class="tag is-post-tag" href="/tags/Neural-Network-TensorFlow/">Neural Network, TensorFlow</a>
                    
                </div>
            
        </div>
        <div class="content" style="overflow: auto">
            <p>This article will peel textsum algorithm and seq2seq with attention mechanism based on the project on TensorFlow/models into very little detail. (<a href="https://github.com/tensorflow/models/tree/master/research/textsum" target="_blank" rel="external">tensorflow/models/research/textsum</a>)And here is the code of my <a href="https://github.com/EstelleHuang666/textsum_tensorflow_usr" target="_blank" rel="external">personal realization</a> of seq2seq with Attention based on the official code, and it turns out to converge quite well.  </p>
<h2 id="Transfer-Text-into-bin-Type"><a href="#Transfer-Text-into-bin-Type" class="headerlink" title="Transfer Text into .bin Type"></a>Transfer Text into .bin Type</h2><p>This part is written in textsum_data_convert.py. The inputs are text file, encoded in utf-8. It has two functions.</p>
<ol>
<li>It outputs .bin file which transform the original unstructured text file into structured .bin file. </li>
<li>It outputs vocab file which counts frequncy of certain word in text files and stores the word as well as its frequency.<br>As for the first function, we have: </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">def _convert_files_to_binary(input_filenames, output_filename):</div><div class="line">  with open(output_filename, &apos;wb&apos;) as writer:</div><div class="line">    for filename in input_filenames:</div><div class="line">      with open(filename, &apos;r&apos;) as f:</div><div class="line">        document = f.read()</div><div class="line">    </div><div class="line">      document_parts = document.split(&apos;\n&apos;, 1)</div><div class="line">      assert len(document_parts) == 2</div><div class="line">    </div><div class="line">      title = &apos;&lt;d&gt;&lt;p&gt;&lt;s&gt;&apos; + document_parts[0] + &apos;&lt;/s&gt;&lt;/p&gt;&lt;/d&gt;&apos;</div><div class="line">      # encode the title into the form of (&apos;UTF-8&apos;), otherwise in tf_example.features.feature[].bytes_list.value.extend()</div><div class="line">      # will report an error.</div><div class="line">      title = title.encode(&apos;utf8&apos;)</div><div class="line">      </div><div class="line">      # body = document_parts[1].decode(&apos;utf8&apos;).replace(&apos;\n&apos;, &apos; &apos;).replace(&apos;\t&apos;, &apos; &apos;)</div><div class="line">      # AttributeError: &apos;str&apos; object has no attribute &apos;decode&apos; -&gt; by Murphy 02.Jan.18</div><div class="line">      try:</div><div class="line">        body = document_parts[1].decode(&apos;utf8&apos;).replace(&apos;\n&apos;, &apos; &apos;).replace(&apos;\t&apos;, &apos; &apos;)</div><div class="line">      except:</div><div class="line">        body = document_parts[1].replace(&apos;\n&apos;, &apos; &apos;).replace(&apos;\t&apos;, &apos; &apos;)</div><div class="line">      sentences = sent_tokenize(body)</div><div class="line">      body = &apos;&lt;d&gt;&lt;p&gt;&apos; + &apos; &apos;.join([&apos;&lt;s&gt;&apos; + sentence + &apos;&lt;/s&gt;&apos; for sentence in sentences]) + &apos;&lt;/p&gt;&lt;/d&gt;&apos;</div><div class="line">      body = body.encode(&apos;utf8&apos;)</div><div class="line">    </div><div class="line">      tf_example = example_pb2.Example()</div><div class="line">      tf_example.features.feature[&apos;article&apos;].bytes_list.value.extend([body])</div><div class="line">      tf_example.features.feature[&apos;abstract&apos;].bytes_list.value.extend([title])</div><div class="line">      tf_example_str = tf_example.SerializeToString()</div><div class="line">      str_len = len(tf_example_str)</div><div class="line">      writer.write(struct.pack(&apos;q&apos;, str_len))</div><div class="line">      writer.write(struct.pack(&apos;%ds&apos; % str_len, tf_example_str))</div></pre></td></tr></table></figure>
<p>It processes all the text files under the path “./data/cnn/stories”. These text files are consist of two parts, “title” and “body” (also known as “abstract” and “article”), which are seperated by the first “\n”. For title and body, both of them are decorated with prefix &lt;\d&gt;&lt;\p&gt;&lt;\s&gt; and postfix &lt;/\s&gt;&lt;/\p&gt;&lt;/\d&gt;. Moreover, body part is seperated into sentences by the fucntion sent_tokenize from nltk package. Every sentence in body is decorated with prefix &lt;\s&gt; and postfix &lt;/\s&gt;. The “/n”s in body are replaced by “/t”s. Then, the processed “title” and “body” are writen into text file through the format of example_pb2.Example(), which is a format imported from tensorflow.core.example. It can simply be preceived as a storing format without special meaning. If you like, you could write your own format to replace example_pb2.Example().</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">def _text_to_vocabulary(input_directories, vocabulary_filename, max_words=200000):</div><div class="line">  filenames = _get_filenames(input_directories)</div><div class="line">    </div><div class="line">  counter = collections.Counter()</div><div class="line">    </div><div class="line">  for filename in filenames:</div><div class="line">    with open(filename, &apos;r&apos;) as f:</div><div class="line">      document = f.read()</div><div class="line">    </div><div class="line">    words = document.split()</div><div class="line">    counter.update(words)</div><div class="line"></div><div class="line">  with open(vocabulary_filename, &apos;w&apos;) as writer:</div><div class="line">    for word, count in counter.most_common(max_words - 2):</div><div class="line">      writer.write(word + &apos; &apos; + str(count) + &apos;\n&apos;)</div><div class="line">    writer.write(&apos;&lt;s&gt; 0\n&apos;)</div><div class="line">    writer.write(&apos;&lt;/s&gt; 0\n&apos;)</div><div class="line">    writer.write(&apos;&lt;UNK&gt; 0\n&apos;)</div><div class="line">    writer.write(&apos;&lt;PAD&gt; 0\n&apos;)</div></pre></td></tr></table></figure>
<p>collections.Counter() works as a collector (or you can see as a dictionary) in the format of {“word1”: frequency1 , “word2”: frequency2 ,…}. By executing “.update” operation and pass a list of words into the Counter, you could let this class automatically update the word-frequency dictionary. At the end, it adds some symbols with special meaning to the Counter. (&lt;\s&gt; &lt;/\s&gt; denote the start and the end of a sentence; <pad> is used to padding the blank to make all of the sentences have same length) </pad></p>
<h2 id="batcher-reader-management-of-input-data"><a href="#batcher-reader-management-of-input-data" class="headerlink" title="batcher_reader: management of input data"></a>batcher_reader: management of input data</h2><p>First, let’s have a glance of the major part of function main:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">vocab = data.Vocab(FLAGS.vocab_path, 1000000)</div><div class="line">  # Check for presence of required special tokens.</div><div class="line">  assert vocab.CheckVocab(data.PAD_TOKEN) &gt; 0</div><div class="line">  assert vocab.CheckVocab(data.UNKNOWN_TOKEN) &gt;= 0</div><div class="line">  assert vocab.CheckVocab(data.SENTENCE_START) &gt; 0</div><div class="line">  assert vocab.CheckVocab(data.SENTENCE_END) &gt; 0</div><div class="line"></div><div class="line">  batch_size = 4</div><div class="line">  if FLAGS.mode == &apos;decode&apos;:</div><div class="line">    batch_size = FLAGS.beam_size</div><div class="line"></div><div class="line">  hps = seq2seq_attention_model.HParams(</div><div class="line">      mode=FLAGS.mode,  # train, eval, decode</div><div class="line">      min_lr=0.001,  # min learning rate.</div><div class="line">      lr=0.015,  # learning rate</div><div class="line">      batch_size=batch_size,</div><div class="line">      enc_layers=1,</div><div class="line">      enc_timesteps=800,</div><div class="line">      dec_timesteps=50,</div><div class="line">      min_input_len=2,  # discard articles/summaries &lt; than this</div><div class="line">      num_hidden=256,  # for rnn cell</div><div class="line">      emb_dim=128,  # If 0, don&apos;t use embedding</div><div class="line">      max_grad_norm=2,</div><div class="line">      num_softmax_samples=4096)  # If 0, no sampled softmax.</div><div class="line"></div><div class="line">  batcher = batch_reader.Batcher(</div><div class="line">      FLAGS.data_path, vocab, hps, FLAGS.article_key,</div><div class="line">      FLAGS.abstract_key, FLAGS.max_article_sentences,</div><div class="line">      FLAGS.max_abstract_sentences, bucketing=FLAGS.use_bucketing,</div><div class="line">      truncate_input=FLAGS.truncate_input)</div><div class="line">  tf.set_random_seed(FLAGS.random_seed)</div><div class="line"></div><div class="line">  if hps.mode == &apos;train&apos;:</div><div class="line">    model = seq2seq_attention_model.Seq2SeqAttentionModel(</div><div class="line">        hps, vocab, num_gpus=FLAGS.num_gpus)</div><div class="line">    _Train(model, batcher)</div><div class="line">  elif hps.mode == &apos;eval&apos;:</div><div class="line">    model = seq2seq_attention_model.Seq2SeqAttentionModel(</div><div class="line">        hps, vocab, num_gpus=FLAGS.num_gpus)</div><div class="line">    _Eval(model, batcher, vocab=vocab)</div><div class="line">  elif hps.mode == &apos;decode&apos;:</div><div class="line">    decode_mdl_hps = hps</div><div class="line">    # Only need to restore the 1st step and reuse it since</div><div class="line">    # we keep and feed in state for each step&apos;s output.</div><div class="line">    decode_mdl_hps = hps._replace(dec_timesteps=1)</div><div class="line">    model = seq2seq_attention_model.Seq2SeqAttentionModel(</div><div class="line">        decode_mdl_hps, vocab, num_gpus=FLAGS.num_gpus)</div><div class="line">    decoder = seq2seq_attention_decode.BSDecoder(model, batcher, hps, vocab)</div><div class="line">    decoder.DecodeLoop()</div></pre></td></tr></table></figure></p>
<p>We can see that main function do the following steps:</p>
<ol>
<li>read in hyperparameters;</li>
<li>use batch_reader to manage the data;</li>
<li>execute training/evaluating/decoding process.</li>
</ol>
<p>For batch_reader, it only contains one class: Batcher. It has following methods:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">class Batcher(object):</div><div class="line">	def NextBatch(self):</div><div class="line"></div><div class="line">	def _FillInputQueue(self):</div><div class="line"></div><div class="line">	def _FillBucketInputQueue(self):</div><div class="line"></div><div class="line">	def _WatchThreads(self):</div><div class="line"></div><div class="line">	def _TextGenerator(self, example_gen):</div><div class="line"></div><div class="line">	def _GetExFeatureText(self, ex, key):</div><div class="line"></div></pre></td></tr></table></figure></p>
<p>We are gonna to dissect these code one by one:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line">def _FillInputQueue(self):</div><div class="line">	&quot;&quot;&quot;Fill input queue with ModelInput.&quot;&quot;&quot;</div><div class="line">	start_id = self._vocab.WordToId(data.SENTENCE_START)</div><div class="line">	end_id = self._vocab.WordToId(data.SENTENCE_END)</div><div class="line">	pad_id = self._vocab.WordToId(data.PAD_TOKEN)</div><div class="line">	input_gen = self._TextGenerator(data.ExampleGen(self._data_path))</div><div class="line">	while True:</div><div class="line">	  (article, abstract) = six.next(input_gen)</div><div class="line">	  article_sentences = [sent.strip() for sent in</div><div class="line">	                       data.ToSentences(article.decode(&apos;utf-8&apos;), include_token=False)]</div><div class="line">	  abstract_sentences = [sent.strip() for sent in</div><div class="line">	                        data.ToSentences(abstract.decode(&apos;utf-8&apos;), include_token=False)]</div><div class="line"></div><div class="line">	  enc_inputs = []</div><div class="line">	  # Use the &lt;s&gt; as the &lt;GO&gt; symbol for decoder inputs.</div><div class="line">	  dec_inputs = [start_id]</div><div class="line"></div><div class="line">	  # Convert first N sentences to word IDs, stripping existing &lt;s&gt; and &lt;/s&gt;.</div><div class="line">	  for i in xrange(min(self._max_article_sentences,</div><div class="line">	                      len(article_sentences))):</div><div class="line">	    enc_inputs += data.GetWordIds(article_sentences[i], self._vocab)</div><div class="line">	  for i in xrange(min(self._max_abstract_sentences,</div><div class="line">	                      len(abstract_sentences))):</div><div class="line">	    dec_inputs += data.GetWordIds(abstract_sentences[i], self._vocab)</div><div class="line"></div><div class="line">	  # Filter out too-short input</div><div class="line">	  if (len(enc_inputs) &lt; self._hps.min_input_len or</div><div class="line">	      len(dec_inputs) &lt; self._hps.min_input_len):</div><div class="line">	    tf.logging.warning(&apos;Drop an example - too short.\nenc:%d\ndec:%d&apos;,</div><div class="line">	                       len(enc_inputs), len(dec_inputs))</div><div class="line">	    continue</div><div class="line"></div><div class="line">	  # If we&apos;re not truncating input, throw out too-long input</div><div class="line">	  if not self._truncate_input:</div><div class="line">	    if (len(enc_inputs) &gt; self._hps.enc_timesteps or</div><div class="line">	        len(dec_inputs) &gt; self._hps.dec_timesteps):</div><div class="line">	      tf.logging.warning(&apos;Drop an example - too long.\nenc:%d\ndec:%d&apos;,</div><div class="line">	                         len(enc_inputs), len(dec_inputs))</div><div class="line">	      continue</div><div class="line">	  # If we are truncating input, do so if necessary</div><div class="line">	  else:</div><div class="line">	    if len(enc_inputs) &gt; self._hps.enc_timesteps:</div><div class="line">	      enc_inputs = enc_inputs[:self._hps.enc_timesteps]</div><div class="line">	    if len(dec_inputs) &gt; self._hps.dec_timesteps:</div><div class="line">	      dec_inputs = dec_inputs[:self._hps.dec_timesteps]</div><div class="line"></div><div class="line">	  # targets is dec_inputs without &lt;s&gt; at beginning, plus &lt;/s&gt; at end</div><div class="line">	  targets = dec_inputs[1:]</div><div class="line">	  targets.append(end_id)</div><div class="line"></div><div class="line">	  # Now len(enc_inputs) should be &lt;= enc_timesteps, and</div><div class="line">	  # len(targets) = len(dec_inputs) should be &lt;= dec_timesteps</div><div class="line"></div><div class="line">	  enc_input_len = len(enc_inputs)</div><div class="line">	  dec_output_len = len(targets)</div><div class="line"></div><div class="line">	  # Pad if necessary</div><div class="line">	  while len(enc_inputs) &lt; self._hps.enc_timesteps:</div><div class="line">	    enc_inputs.append(pad_id)</div><div class="line">	  while len(dec_inputs) &lt; self._hps.dec_timesteps:</div><div class="line">	    dec_inputs.append(end_id)</div><div class="line">	  while len(targets) &lt; self._hps.dec_timesteps:</div><div class="line">	    targets.append(end_id)</div><div class="line"></div><div class="line">	  element = ModelInput(enc_inputs, dec_inputs, targets, enc_input_len,</div><div class="line">	                       dec_output_len, &apos; &apos;.join(article_sentences),</div><div class="line">	                       &apos; &apos;.join(abstract_sentences))</div><div class="line">	  self._input_queue.put(element)</div></pre></td></tr></table></figure></p>
<p>self._vocab is a class defined in data.py. To understand it briefly, you can view it as composed of two dictionary: _word_to_id and _id_to_word. In addition, this class provides methods related to “word to id” and “id to word” processes. As for self._TextGenerator method, it uses yield instead of return to deal with the huge memory usage. To master yield, you must understand that when you call the function, the code you have written in the function body does not run. The function only returns the generator object. To understand better, please refer to <a href="https://pythontips.com/2013/09/29/the-python-yield-keyword-explained/" target="_blank" rel="external">this article</a>. </p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">def _TextGenerator(self, example_gen):</div><div class="line">	&quot;&quot;&quot;Generates article and abstract text from tf.Example.&quot;&quot;&quot;</div><div class="line">	while True:</div><div class="line">	  e = six.next(example_gen)</div><div class="line">	  try:</div><div class="line">	    article_text = self._GetExFeatureText(e, self._article_key) //return ex.features.feature[key].bytes_list.value[0]</div><div class="line">	    abstract_text = self._GetExFeatureText(e, self._abstract_key)</div><div class="line">	  except ValueError:</div><div class="line">	    tf.logging.error(&apos;Failed to get article or abstract from example&apos;)</div><div class="line">	    continue</div><div class="line"></div><div class="line">	  yield (article_text, abstract_text)</div></pre></td></tr></table></figure>
<p>six.next(example_gen) generates a tf.Example type if data of the following sample format. It’s long, but you only need to have a glanpse of its basic format.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">features &#123;</div><div class="line">  feature &#123;</div><div class="line">    key: &quot;abstract&quot;</div><div class="line">    value &#123;</div><div class="line">      bytes_list &#123;</div><div class="line">        value: &quot;&lt;d&gt;&lt;p&gt;&lt;s&gt;(CNN) -- Republican presidential contender Michele Bachmann defended her position on gay rights, the HPV vaccine and the debt ceiling as she made her debut on \&quot;The Tonight Show with Jay Leno.\&quot;&lt;/s&gt;&lt;/p&gt;&lt;/d&gt;&quot;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  feature &#123;</div><div class="line">    key: &quot;article&quot;</div><div class="line">    value &#123;</div><div class="line">      bytes_list &#123;</div><div class="line">        value: &quot;&lt;d&gt;&lt;p&gt;&lt;s&gt; Bachmann has more usually been the butt of Leno\&apos;s jokes -- a point he made as he thanked her for being a good sport.&lt;/s&gt; &lt;s&gt;\&quot;We\&apos;ve done a million jokes.&lt;/s&gt; &lt;s&gt;Hopefully, you haven\&apos;t been ... watching any of them,\&quot; he said, as she joined him on set.&lt;/s&gt; &lt;s&gt;But Leno largely skipped the jokes Friday as he quizzed the Minnesota congresswoman on her political positions.&lt;/s&gt; &lt;s&gt;First up was the issue of the HPV vaccine, a subject on which Bachmann hit fellow Republican contender and Texas Gov.&lt;/s&gt; &lt;s&gt;Rick Perry hard in this week\&apos;s CNN/Tea Party Republican Debate.&lt;/s&gt; &lt;s&gt;Perry signed an executive order in 2007 that required Texas schoolgirls to receive vaccinations against the sexually transmitted HPV, although it wasn\&apos;t implemented.&lt;/s&gt; &lt;s&gt;Bachmann told Leno that Perry\&apos;s action had been \&quot;an abuse of executive power\&quot; and had sparked concern over \&quot;crony capitalism,\&quot; an apparent reference to the fact that a former Perry aide was a top lobbyist for Merck, the manufacturer for the HPV vaccine.&lt;/s&gt; &lt;s&gt;Merck donated to Perry\&apos;s campaign fund.&lt;/s&gt; &lt;s&gt;She added: \&quot;The concern is that there\&apos;s, you know, potentially side effects that can come with something like that.&lt;/s&gt; &lt;s&gt;But it gives a false sense of assurance to a young woman when she has that that if she\&apos;s sexually active that she doesn\&apos;t have to worry about sexually transmitted diseases.\&quot;&lt;/s&gt; &lt;s&gt;Leno responded: \&quot;Well, I don\&apos;t know if it gives assurance.&lt;/s&gt; &lt;s&gt;It can prevent cervical cancer; correct?\&quot;&lt;/s&gt; &lt;s&gt;He then pressed Bachmann over comments she made earlier this week in which she said a woman had approached the congresswoman to say her daughter had suffered \&quot;mental retardation\&quot; as a result of receiving the vaccination.&lt;/s&gt; &lt;s&gt;There had been no recorded cases of such side effects despite 30 million people receiving the jab, Leno pointed out.&lt;/s&gt; &lt;s&gt;\&quot;I wasn\&apos;t speaking as a doctor.&lt;/s&gt; &lt;s&gt;I wasn\&apos;t speaking as a scientist.&lt;/s&gt; &lt;s&gt;I was just relating what this woman said,\&quot; Bachmann replied.&lt;/s&gt; &lt;s&gt;The former tax attorney and mother of five, who won the Iowa straw poll last month but has seen her poll ratings slide since Perry entered the race, also defended two clinics she runs with her husband, offering what she said was a Christian counseling service.&lt;/s&gt; &lt;s&gt;The clinics have come under fire over claims they use a controversial therapy that encourages gay and lesbian patients to change their sexual orientation.&lt;/s&gt; &lt;s&gt;Asking Bachmann why gay people shouldn\&apos;t have the right to be happily married, Leno said: \&quot;That whole \&apos;pray the gay away\&apos; thing, What?&lt;/s&gt; &lt;s&gt;I don\&apos;t get that.\&quot;&lt;/s&gt; &lt;s&gt;Bachmann said the clinics did not discriminate, but repeated her position that marriage should be between a man and a woman.&lt;/s&gt; &lt;s&gt;Quizzed on her opposition to raising the debt ceiling, Bachmann said she would have taken the same position whether it had been President Barack Obama or George W. Bush in power.&lt;/s&gt; &lt;s&gt;On Afghanistan, Bachmann failed to answer whether she thought American forces should withdraw, but paid tribute to the \&quot;unbelievable job\&quot; done by U.S. service men and women there.&lt;/s&gt; &lt;s&gt;Leno\&apos;s final question was whom Bachmann would pick as a running mate if she wins the Republican nomination, suggesting she might want someone more moderate to balance her views.&lt;/s&gt; &lt;s&gt;She joked: \&quot;Well, you\&apos;re taken.&lt;/s&gt; &lt;s&gt;You don\&apos;t want a cut in pay, so what can I say?\&quot;&lt;/s&gt; &lt;s&gt;Leno replied: \&quot;Well, we\&apos;d probably have an argument over that gay thing.\&quot;&lt;/s&gt; &lt;s&gt;@highlight  Bachmann continues her criticism of Rick Perry over the HPV vaccine  @highlight  Leno presses Bachmann on gay marriage and the counseling clinics she runs  @highlight  The presidential hopeful says her opposition to raising the debt ceiling was not political  @highlight  Bachmann declines to say who her running mate would be&lt;/s&gt;&lt;/p&gt;&lt;/d&gt;&quot;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>

        </div>
        <div class="post-reply">
            
                <!-- 来必力City版安装代码 -->
                <div id="lv-container" data-id="city" data-uid="MTAyMC8yOTE4Ni81NzUz">
                    <script type="text/javascript">
                        (function(d, s) {
                            var j, e = d.getElementsByTagName(s)[0];

                            if (typeof LivereTower === 'function') { return; }

                            j = d.createElement(s);
                            j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                            j.async = true;

                            e.parentNode.insertBefore(j, e);
                        })(document, 'script');
                    </script>
                    <noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
                </div>
                <!-- City版安装代码已完成 -->
            
            
        </div>
    </div>
</section>
<script>
    // 获取第一张图, 用以当封面背景图
    var img = document.querySelectorAll('img')[1]

    if (img) {
        var header_box = document.querySelector('#header_box')
        header_box.style.backgroundImage = 'url('+ img.src +')'
    }
</script>
      </div>
  </div>
  <style>
  #footer {
    min-height: 10vh;
    background: black;
    color: #fff;
  }

  #footer a {
    color: #e1e1e1;
  }
</style>
<footer id="footer" class="has-text-centered is-flex center">
  <div class="container has-padding">
    <div>
      <div>
        <!--请您保留作者署名, 主题制作来之不易-->
        Theme by <a href="http://haojen.github.io/">Haojen Ma</a>
        <br>
        Copyright © Yuanhanqing Huang 2019
        <br>
        Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      </div>
    </div>
  </div>
</footer>

<script src="/js/search_core.js"></script>
<script src="/js/script.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>